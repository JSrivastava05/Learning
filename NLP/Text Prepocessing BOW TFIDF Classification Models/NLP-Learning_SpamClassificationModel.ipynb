{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import nltk\n",
    "# nltk.download('all')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tokenization Process. Paragraph to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "para='''Here are many examples of short stories for you to read online. Online has become another leg in our life. WE have to take that into account so that we will go along the growth of the science and technology. Computer has revolutionalised our world. The people have started to see another world. What we were has become history. The twentieth century has become remote history. The IT companies and other computer-based companies have outperformed other traditional companies which have been there for a long time. Accuracy has become the most used word among the people. Telecommunication has become very very cheap affair all over the world. All these achievements are possible because of Computer and the Internet. Reading short stories online has become our favorite pastime.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentences in the above paragraph :  12\n",
      "['Here are many examples of short stories for you to read online.', 'Online has become another leg in our life.', 'WE have to take that into account so that we will go along the growth of the science and technology.', 'Computer has revolutionalised our world.', 'The people have started to see another world.', 'What we were has become history.', 'The twentieth century has become remote history.', 'The IT companies and other computer-based companies have outperformed other traditional companies which have been there for a long time.', 'Accuracy has become the most used word among the people.', 'Telecommunication has become very very cheap affair all over the world.', 'All these achievements are possible because of Computer and the Internet.', 'Reading short stories online has become our favorite pastime.']\n"
     ]
    }
   ],
   "source": [
    "sentences=nltk.sent_tokenize(para)\n",
    "print('Total number of sentences in the above paragraph : ' , len(sentences))\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tokenization Process. Paragraph to Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in the above pragraph :  139\n",
      "['Here', 'are', 'many', 'examples', 'of', 'short', 'stories', 'for', 'you', 'to', 'read', 'online', '.', 'Online', 'has', 'become', 'another', 'leg', 'in', 'our', 'life', '.', 'WE', 'have', 'to', 'take', 'that', 'into', 'account', 'so', 'that', 'we', 'will', 'go', 'along', 'the', 'growth', 'of', 'the', 'science', 'and', 'technology', '.', 'Computer', 'has', 'revolutionalised', 'our', 'world', '.', 'The', 'people', 'have', 'started', 'to', 'see', 'another', 'world', '.', 'What', 'we', 'were', 'has', 'become', 'history', '.', 'The', 'twentieth', 'century', 'has', 'become', 'remote', 'history', '.', 'The', 'IT', 'companies', 'and', 'other', 'computer-based', 'companies', 'have', 'outperformed', 'other', 'traditional', 'companies', 'which', 'have', 'been', 'there', 'for', 'a', 'long', 'time', '.', 'Accuracy', 'has', 'become', 'the', 'most', 'used', 'word', 'among', 'the', 'people', '.', 'Telecommunication', 'has', 'become', 'very', 'very', 'cheap', 'affair', 'all', 'over', 'the', 'world', '.', 'All', 'these', 'achievements', 'are', 'possible', 'because', 'of', 'Computer', 'and', 'the', 'Internet', '.', 'Reading', 'short', 'stories', 'online', 'has', 'become', 'our', 'favorite', 'pastime', '.']\n"
     ]
    }
   ],
   "source": [
    "words= nltk.word_tokenize(para)\n",
    "print('Total number of words in the above pragraph : ' , len(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Text Normalization : Convert the sentences into LOWER CASE, except for cases where UPPER case is necessary example : US/us etc\n",
    "### 4. Stop word: is, a, am, are etc. Not required for processing so it should be removed from text\n",
    "\n",
    "### 5. Stemming and Lemmatization: Stemming and Lemmatization both generate the root form of the inflected words. The difference is that stem might not be an actual word whereas, lemma is an actual language word. \n",
    "- Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma. Stemming follows an algorithm with steps to perform on the words which makes it faster.\n",
    "- Lemmatization : The process of reducing the different forms of a word to one single form with the use of Dictionary. Slower processing than stemming\n",
    "\n",
    "The main advantage of lemmatization is that it takes into consideration the context of the word to determine which is the intended meaning the user is looking for. This process allows to decrease noise and speed up the user's task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mani exampl short stori read onlin .',\n",
       " 'onlin becom anoth leg life .',\n",
       " 'take account go along growth scienc technolog .',\n",
       " 'comput revolutionalis world .',\n",
       " 'peopl start see anoth world .',\n",
       " 'becom histori .',\n",
       " 'twentieth centuri becom remot histori .',\n",
       " 'compani computer-bas compani outperform tradit compani long time .',\n",
       " 'accuraci becom use word among peopl .',\n",
       " 'telecommun becom cheap affair world .',\n",
       " 'achiev possibl comput internet .',\n",
       " 'read short stori onlin becom favorit pastim .']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "para='''Here are many examples of short stories for you to read online. Online has become another leg in our life. WE have to take that into account so that we will go along the growth of the science and technology. Computer has revolutionalised our world. The people have started to see another world. What we were has become history. The twentieth century has become remote history. The IT companies and other computer-based companies have outperformed other traditional companies which have been there for a long time. Accuracy has become the most used word among the people. Telecommunication has become very very cheap affair all over the world. All these achievements are possible because of Computer and the Internet. Reading short stories online has become our favorite pastime.'''\n",
    "sentences=nltk.sent_tokenize(para.lower())\n",
    "stemmer=PorterStemmer()\n",
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words=[stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i]=' '.join(words)\n",
    "sentences # Stemming has been applied to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main problem with stemming is that its does not provide proper words with meaning.  \n",
    "### To overcome this, the alternative is lemmetization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['many example short story read online .',\n",
       " 'online become another leg life .',\n",
       " 'take account go along growth science technology .',\n",
       " 'computer revolutionalised world .',\n",
       " 'people started see another world .',\n",
       " 'become history .',\n",
       " 'twentieth century become remote history .',\n",
       " 'company computer-based company outperformed traditional company long time .',\n",
       " 'accuracy become used word among people .',\n",
       " 'telecommunication become cheap affair world .',\n",
       " 'achievement possible computer internet .',\n",
       " 'reading short story online become favorite pastime .']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "para='''Here are many examples of short stories for you to read online. Online has become another leg in our life. WE have to take that into account so that we will go along the growth of the science and technology. Computer has revolutionalised our world. The people have started to see another world. What we were has become history. The twentieth century has become remote history. The IT companies and other computer-based companies have outperformed other traditional companies which have been there for a long time. Accuracy has become the most used word among the people. Telecommunication has become very very cheap affair all over the world. All these achievements are possible because of Computer and the Internet. Reading short stories online has become our favorite pastime.'''\n",
    "sentences=nltk.sent_tokenize(para.lower())\n",
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words=[lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i]=' '.join(words)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization overcomes the issue with stemming and provides words with correct context or meanings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words Vecotization with stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mani exampl short stori read onlin', 'onlin becom anoth leg life', 'take account go along growth scienc technolog', 'comput revolutionalis world', 'peopl start see anoth world', 'becom histori', 'twentieth centuri becom remot histori', 'compani comput base compani outperform tradit compani long time', 'accuraci becom use word among peopl', 'telecommun becom cheap affair world', 'achiev possibl comput internet', 'read short stori onlin becom favorit pastim']\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 1 0 1\n",
      "  0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      "  1 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0\n",
      "  0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      "  0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 3 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 1 1 0 0 0 0]\n",
      " [0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 1 1 0]\n",
      " [0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 1 0 0 0 0 0 1]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 1 0 1\n",
      "  0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "para='''Here are many examples of short stories for you to read online. Online has become another leg in our life. WE have to take that into account so that we will go along the growth of the science and technology. Computer has revolutionalised our world. The people have started to see another world. What we were has become history. The twentieth century has become remote history. The IT companies and other computer-based companies have outperformed other traditional companies which have been there for a long time. Accuracy has become the most used word among the people. Telecommunication has become very very cheap affair all over the world. All these achievements are possible because of Computer and the Internet. Reading short stories online has become our favorite pastime.'''\n",
    "ps=PorterStemmer()\n",
    "wordnet=WordNetLemmatizer()\n",
    "sentences=nltk.sent_tokenize(para)\n",
    "stem_sentences=[]\n",
    "for i in range(len(sentences)):\n",
    "    review=re.sub('[^a-zA-Z]',' ',sentences[i]) # replace everything apart from a-z with spaces\n",
    "    review=review.lower()\n",
    "    review=review.split()\n",
    "    review=[ps.stem(word) for word in review if word not in set(stopwords.words('english'))]\n",
    "    review=' '.join(review)\n",
    "    stem_sentences.append(review)\n",
    "print(stem_sentences)\n",
    "cv=CountVectorizer(max_features=1500)\n",
    "X=cv.fit_transform(stem_sentences).toarray()\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words Vecotization with Lematization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['many example short story read online', 'online become another leg life', 'take account go along growth science technology', 'computer revolutionalised world', 'people started see another world', 'become history', 'twentieth century become remote history', 'company computer based company outperformed traditional company long time', 'accuracy become used word among people', 'telecommunication become cheap affair world', 'achievement possible computer internet', 'reading short story online become favorite pastime']\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 1 0\n",
      "  1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      "  0 1 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1\n",
      "  0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      "  0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 3 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 1 1 0 0 0 0]\n",
      " [0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 1 1 0]\n",
      " [0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 1 0 0 0 0 0 1]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 1 0\n",
      "  1 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "para='''Here are many examples of short stories for you to read online. Online has become another leg in our life. WE have to take that into account so that we will go along the growth of the science and technology. Computer has revolutionalised our world. The people have started to see another world. What we were has become history. The twentieth century has become remote history. The IT companies and other computer-based companies have outperformed other traditional companies which have been there for a long time. Accuracy has become the most used word among the people. Telecommunication has become very very cheap affair all over the world. All these achievements are possible because of Computer and the Internet. Reading short stories online has become our favorite pastime.'''\n",
    "ps=PorterStemmer()\n",
    "wordnet=WordNetLemmatizer()\n",
    "sentences=nltk.sent_tokenize(para)\n",
    "lem_sentences=[]\n",
    "for i in range(len(sentences)):\n",
    "    review=re.sub('[^a-zA-Z]',' ',sentences[i]) # replace everything apart from a-z with spaces\n",
    "    review=review.lower()\n",
    "    review=review.split()\n",
    "    review=[wordnet.lemmatize(word) for word in review if word not in set(stopwords.words('english'))]\n",
    "    review=' '.join(review)\n",
    "    lem_sentences.append(review)\n",
    "print(lem_sentences)\n",
    "cv=CountVectorizer(max_features=1500)\n",
    "X1=cv.fit_transform(lem_sentences).toarray()\n",
    "print(X1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To overcome disadvantages of bag of words, NLP has TF-IDF (term frequeny and inverse docuement frequency) alternative algo\n",
    "- term frequency TF = (number of repetition of words in a sentence)/(total number of words in the sentence) \n",
    "- inverse docuement frequency IDF = log((number of sentences/number of sentences containing the words)) \n",
    "- TF vectorizes the words and IDF provides them with weight "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.43799565 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.43799565\n",
      "  0.         0.43799565 0.         0.         0.         0.\n",
      "  0.         0.37615575 0.         0.         0.         0.\n",
      "  0.37615575 0.         0.37615575 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.44097015 0.         0.28947705 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.51346552 0.51346552 0.         0.\n",
      "  0.44097015 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.37796447 0.         0.         0.         0.37796447 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.37796447 0.37796447\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.37796447 0.\n",
      "  0.         0.         0.         0.37796447 0.37796447 0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.56467934 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.65751246 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.49881319]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.41352692 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.48151063 0.\n",
      "  0.         0.         0.         0.         0.         0.48151063\n",
      "  0.         0.48151063 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.36529171]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.5487762  0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.83596931 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.27995372 0.49657334 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.42646296 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.49657334 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.49657334 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.25819889 0.         0.         0.         0.77459667\n",
      "  0.         0.25819889 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.25819889 0.\n",
      "  0.         0.         0.25819889 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.25819889 0.25819889 0.         0.         0.         0.        ]\n",
      " [0.         0.43364311 0.         0.         0.         0.43364311\n",
      "  0.         0.         0.24447547 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.43364311\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.43364311 0.43364311 0.        ]\n",
      " [0.         0.         0.         0.50680079 0.         0.\n",
      "  0.         0.         0.28571966 0.         0.50680079 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.50680079\n",
      "  0.         0.         0.         0.         0.         0.38447776]\n",
      " [0.         0.         0.51725663 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.444226   0.         0.         0.         0.         0.\n",
      "  0.         0.51725663 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.51725663 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.24562761 0.         0.         0.\n",
      "  0.         0.         0.         0.43568673 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.37417283 0.         0.         0.43568673 0.         0.\n",
      "  0.         0.37417283 0.         0.         0.         0.\n",
      "  0.37417283 0.         0.37417283 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]]\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.44496576 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.44496576 0.33756743\n",
      "  0.         0.         0.         0.         0.44496576 0.\n",
      "  0.         0.         0.         0.         0.38214176 0.\n",
      "  0.38214176 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.45070177 0.         0.29586541 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.52479701 0.52479701 0.         0.         0.39813036\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.37796447 0.         0.         0.         0.37796447 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.37796447 0.37796447 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.37796447 0.         0.         0.\n",
      "  0.         0.37796447 0.37796447 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.51725858 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.68182632 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.51725858]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.42671296 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.42671296 0.         0.         0.\n",
      "  0.         0.         0.         0.49686445 0.         0.49686445\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.37693969]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.5487762  0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.83596931\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.27995372 0.49657334 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.42646296\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.49657334 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.49657334 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.26193156 0.         0.         0.         0.78579467\n",
      "  0.19871093 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.26193156 0.         0.\n",
      "  0.26193156 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.26193156 0.26193156\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.44475664 0.         0.         0.         0.44475664\n",
      "  0.         0.         0.25074096 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.38196217 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.44475664 0.44475664 0.        ]\n",
      " [0.         0.         0.         0.50680079 0.         0.\n",
      "  0.         0.         0.28571966 0.         0.50680079 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.50680079 0.         0.\n",
      "  0.         0.         0.         0.38447776]\n",
      " [0.         0.         0.52884669 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.4012026  0.         0.         0.         0.         0.\n",
      "  0.52884669 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.52884669 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.24331959 0.         0.         0.\n",
      "  0.         0.         0.43159283 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.32742223\n",
      "  0.         0.43159283 0.         0.         0.         0.43159283\n",
      "  0.         0.         0.         0.         0.37065694 0.\n",
      "  0.37065694 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "para='''Here are many examples of short stories for you to read online. Online has become another leg in our life. WE have to take that into account so that we will go along the growth of the science and technology. Computer has revolutionalised our world. The people have started to see another world. What we were has become history. The twentieth century has become remote history. The IT companies and other computer-based companies have outperformed other traditional companies which have been there for a long time. Accuracy has become the most used word among the people. Telecommunication has become very very cheap affair all over the world. All these achievements are possible because of Computer and the Internet. Reading short stories online has become our favorite pastime.'''\n",
    "# clean the text\n",
    "sentences=nltk.sent_tokenize(para)\n",
    "ps=PorterStemmer()\n",
    "wn=WordNetLemmatizer()\n",
    "review_stem=[]\n",
    "review_lem=[]\n",
    "\n",
    "# TFIDF with Stemming\n",
    "for i in range(len(sentences)):\n",
    "    review=re.sub('^a-zA-Z', ' ',sentences[i])\n",
    "    review=review.lower()\n",
    "    review=review.split()\n",
    "    review=[ps.stem(word) for word in review if word not in set(stopwords.words('english'))]\n",
    "    review=' '.join(review)\n",
    "    review_stem.append(review)\n",
    "cv=TfidfVectorizer()\n",
    "X=cv.fit_transform(review_stem).toarray()\n",
    "print(X)\n",
    "# TFIDF with Lemmatization\n",
    "for i in range(len(sentences)):\n",
    "    review=re.sub('^a-zA-Z', ' ',sentences[i])\n",
    "    review=review.lower()\n",
    "    review=review.split()\n",
    "    review=[wn.lemmatize(word) for word in review if word not in set(stopwords.words('english'))]\n",
    "    review=' '.join(review)\n",
    "    review_lem.append(review)\n",
    "cv=TfidfVectorizer()\n",
    "X1=cv.fit_transform(review_lem).toarray()\n",
    "print(X1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (confusion_matrix, precision_score, recall_score,roc_curve,auc, accuracy_score, fbeta_score,\n",
    "roc_auc_score, brier_score_loss, plot_precision_recall_curve, average_precision_score)\n",
    "\n",
    "def eval(actual, pred, prob=None):\n",
    "    tn, fp, fn, tp = confusion_matrix(actual, pred).ravel()\n",
    "    print(\"ACCURACY : \"+str(round(accuracy_score(actual, pred), 5)))\n",
    "    # if the model says positive, what percent of them are not positive\n",
    "    print(\"FALSE POSITIVE RATE : \" + str(round(fp/(tn+fp),5)))\n",
    "    # if the model says positive, what percent of them are actually positive\n",
    "    print(\"PRECISION : \" + str(round(tp/(tp+fp),5)))\n",
    "    # within the delinquent customers, what percentage did the model catch\n",
    "    print(\"RECALL : \" + str(round(tp/(tp+fn),5)))\n",
    "    print(\"F(0.5) SCORE : \" + str(round(fbeta_score(actual, pred, 0.5),5)))\n",
    "    print(\"F(1) SCORE : \" + str(round(fbeta_score(actual, pred, 1),5)))\n",
    "    if prob is not None:\n",
    "        print(\"AUC : \" + str(round(roc_auc_score(actual,prob),5)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>messeges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                           messeges\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "messeges=pd.read_csv(r'C:\\Users\\jaysriva\\Documents\\Learning\\NLP\\SpamCollectionDataset\\SMSSpamCollection', sep='\\t', names=['label','messeges'])\n",
    "messeges.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. TF-IDF with Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label column\n",
    "y=pd.get_dummies(messeges.label) # Spam-ham column. selecting only spam column, 0 is ham, 1 is spam the dependent feature\n",
    "y=y.iloc[:,1].values \n",
    "\n",
    "# Data cleaning and Preprocessing\n",
    "ps=PorterStemmer()\n",
    "wn=WordNetLemmatizer()\n",
    "review_lem=[]\n",
    "\n",
    "for i in range(len(messeges)):\n",
    "    review=re.sub('^a-zA-Z', ' ', messeges.messeges[i])\n",
    "    review=review.lower()\n",
    "    review=review.split()\n",
    "    review=[wn.lemmatize(word) for word in review if word not in stopwords.words('english')]\n",
    "    review=' '.join(review)\n",
    "    review_lem.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance with Tf-IDF, Lemmatization - \n",
      "ACCURACY : 0.98296\n",
      "FALSE POSITIVE RATE : 0.0\n",
      "PRECISION : 1.0\n",
      "RECALL : 0.87075\n",
      "F(0.5) SCORE : 0.97117\n",
      "F(1) SCORE : 0.93091\n"
     ]
    }
   ],
   "source": [
    "tv=TfidfVectorizer(max_features=5000) # Top frequent 5000 words from review\n",
    "X=tv.fit_transform(review_lem).toarray()\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=369)\n",
    "spam_detect_model=MultinomialNB().fit(X_train,y_train)\n",
    "y_pred=spam_detect_model.predict(X_test)\n",
    "print('Model Performance with Tf-IDF, Lemmatization - ')\n",
    "eval(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Bag of Words with Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance with BOW, Lemmatization - \n",
      "ACCURACY : 0.98296\n",
      "FALSE POSITIVE RATE : 0.0\n",
      "PRECISION : 1.0\n",
      "RECALL : 0.87075\n",
      "F(0.5) SCORE : 0.97117\n",
      "F(1) SCORE : 0.93091\n"
     ]
    }
   ],
   "source": [
    "tv=TfidfVectorizer(max_features=5000) # Top frequent 5000 words from review\n",
    "X=tv.fit_transform(review_lem).toarray()\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=369)\n",
    "spam_detect_model=MultinomialNB().fit(X_train,y_train)\n",
    "y_pred=spam_detect_model.predict(X_test)\n",
    "print('Model Performance with BOW, Lemmatization - ')\n",
    "eval(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. TF-IDF with Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label column\n",
    "y=pd.get_dummies(messeges.label) # Spam-ham column. selecting only spam column, 0 is ham, 1 is spam the dependent feature\n",
    "y=y.iloc[:,1].values \n",
    "\n",
    "# Data cleaning and Preprocessing\n",
    "ps=PorterStemmer()\n",
    "review_stem=[]\n",
    "\n",
    "for i in range(len(messeges)):\n",
    "    review=re.sub('^a-zA-Z', ' ', messeges.messeges[i])\n",
    "    review=review.lower()\n",
    "    review=review.split()\n",
    "    review=[ps.stem(word) for word in review if word not in stopwords.words('english')]\n",
    "    review=' '.join(review)\n",
    "    review_stem.append(review)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Bag of Words with Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance with Tf-IDF- \n",
      "ACCURACY : 0.99103\n",
      "FALSE POSITIVE RATE : 0.00517\n",
      "PRECISION : 0.96599\n",
      "RECALL : 0.96599\n",
      "F(0.5) SCORE : 0.96599\n",
      "F(1) SCORE : 0.96599\n"
     ]
    }
   ],
   "source": [
    "cv=CountVectorizer(max_features=5000) # Top frequent 5000 words from review\n",
    "X=cv.fit_transform(review_stem).toarray()\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=369)\n",
    "spam_detect_model=MultinomialNB().fit(X_train,y_train)\n",
    "y_pred=spam_detect_model.predict(X_test)\n",
    "print('Model Performance with Tf-IDF, Stemming- ')\n",
    "eval(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance with BOW, Stemming- \n",
      "ACCURACY : 0.98206\n",
      "FALSE POSITIVE RATE : 0.0\n",
      "PRECISION : 1.0\n",
      "RECALL : 0.86395\n",
      "F(0.5) SCORE : 0.96947\n",
      "F(1) SCORE : 0.92701\n"
     ]
    }
   ],
   "source": [
    "tv=TfidfVectorizer(max_features=5000) # Top frequent 5000 words from review\n",
    "X=tv.fit_transform(review_stem).toarray()\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=369)\n",
    "spam_detect_model=MultinomialNB().fit(X_train,y_train)\n",
    "y_pred=spam_detect_model.predict(X_test)\n",
    "print('Model Performance with BOW, Stemming- ')\n",
    "eval(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue with BOW and TFIDF: \n",
    "- 1. no symantic info is stored (symantic info : order, relationship etc)\n",
    "- 2. Chances of overfitting\n",
    "\n",
    "###  To overcome above issue word2vec is used in which word is represented as a vector of 32 or more dimentions. and symantic information and relations between different words is also preserved\n",
    "- Word2Vec can be used to find out the relations between words in a dataset, compute the similarity between them, or use the vector representation of those words as input for other applications such as text classification or clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\jaysriva\\anaconda3\\lib\\site-packages (4.0.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\jaysriva\\anaconda3\\lib\\site-packages (from gensim) (3.0.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\jaysriva\\anaconda3\\lib\\site-packages (from gensim) (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\jaysriva\\anaconda3\\lib\\site-packages (from gensim) (1.19.2)\n",
      "Requirement already satisfied: Cython==0.29.21 in c:\\users\\jaysriva\\anaconda3\\lib\\site-packages (from gensim) (0.29.21)\n",
      "Requirement already satisfied: requests in c:\\users\\jaysriva\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.24.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jaysriva\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\jaysriva\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\jaysriva\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\jaysriva\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2.10)\n"
     ]
    }
   ],
   "source": [
    "! pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "para='''Here are many examples of short stories for you to read online. Online has become another leg in our life. WE have to take that into account so that we will go along the growth of the science and technology. Computer has revolutionalised our world. The people have started to see another world. What we were has become history. The twentieth century has become remote history. The IT companies and other computer-based companies have outperformed other traditional companies which have been there for a long time. Accuracy has become the most used word among the people. Telecommunication has become very very cheap affair all over the world. All these achievements are possible because of Computer and the Internet. Reading short stories online has become our favorite pastime.'''\n",
    "text=re.sub('^a-zA-Z',' ',para)\n",
    "text=re.sub(r'\\[[0-9]*\\]',' ',text)\n",
    "text=re.sub(r'\\s+',' ',text)\n",
    "text=text.lower()\n",
    "text=re.sub(r'\\d',' ',text)\n",
    "text=re.sub(r'\\s+',' ',text)\n",
    "sentences=nltk.sent_tokenize(text)\n",
    "sentences=[nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "for i in range(len(sentences)):\n",
    "    sentences[i]=[word for word in sentences[i] if word not in set(stopwords.words('english'))]\n",
    "model=Word2Vec(sentences,min_count=1) # min_count is word count. If word count is 1 (present), do this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': 0,\n",
       " 'become': 1,\n",
       " 'world': 2,\n",
       " 'online': 3,\n",
       " 'companies': 4,\n",
       " 'history': 5,\n",
       " 'short': 6,\n",
       " 'stories': 7,\n",
       " 'another': 8,\n",
       " 'computer': 9,\n",
       " 'people': 10,\n",
       " 'growth': 11,\n",
       " 'revolutionalised': 12,\n",
       " 'technology': 13,\n",
       " 'science': 14,\n",
       " 'pastime': 15,\n",
       " 'along': 16,\n",
       " 'go': 17,\n",
       " 'account': 18,\n",
       " 'life': 19,\n",
       " 'leg': 20,\n",
       " 'read': 21,\n",
       " 'examples': 22,\n",
       " 'take': 23,\n",
       " 'see': 24,\n",
       " 'started': 25,\n",
       " 'favorite': 26,\n",
       " 'reading': 27,\n",
       " 'internet': 28,\n",
       " 'possible': 29,\n",
       " 'achievements': 30,\n",
       " 'affair': 31,\n",
       " 'cheap': 32,\n",
       " 'telecommunication': 33,\n",
       " 'among': 34,\n",
       " 'word': 35,\n",
       " 'used': 36,\n",
       " 'accuracy': 37,\n",
       " 'time': 38,\n",
       " 'long': 39,\n",
       " 'traditional': 40,\n",
       " 'outperformed': 41,\n",
       " 'computer-based': 42,\n",
       " 'remote': 43,\n",
       " 'century': 44,\n",
       " 'twentieth': 45,\n",
       " 'many': 46}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words=model.wv.key_to_index\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.7702928e-03,  8.1651136e-03,  1.2809705e-03,  5.0975773e-03,\n",
       "        1.4081288e-03, -6.4551616e-03, -1.4280510e-03,  6.4491653e-03,\n",
       "       -4.6173073e-03, -3.9930656e-03,  4.9244044e-03,  2.7130984e-03,\n",
       "       -1.8479753e-03, -2.8769446e-03,  6.0107303e-03, -5.7167388e-03,\n",
       "       -3.2367038e-03, -6.4878250e-03, -4.2346334e-03, -8.5809948e-03,\n",
       "       -4.4697905e-03, -8.5112313e-03,  1.4037776e-03, -8.6181974e-03,\n",
       "       -9.9166557e-03, -8.2016252e-03, -6.7726658e-03,  6.6805840e-03,\n",
       "        3.7845564e-03,  3.5616636e-04, -2.9579829e-03, -7.4283220e-03,\n",
       "        5.3341867e-04,  4.9989222e-04,  1.9561767e-04,  8.5259438e-04,\n",
       "        7.8633073e-04, -6.8161491e-05, -8.0070542e-03, -5.8702733e-03,\n",
       "       -8.3829118e-03, -1.3120436e-03,  1.8206357e-03,  7.4171280e-03,\n",
       "       -1.9634271e-03, -2.3252917e-03,  9.4871549e-03,  7.9703328e-05,\n",
       "       -2.4045228e-03,  8.6048460e-03,  2.6870037e-03, -5.3439736e-03,\n",
       "        6.5881060e-03,  4.5101522e-03, -7.0544672e-03, -3.2317400e-04,\n",
       "        8.3448651e-04,  5.7473565e-03, -1.7176557e-03, -2.8065301e-03,\n",
       "        1.7484308e-03,  8.4717036e-04,  1.1928272e-03, -2.6342822e-03,\n",
       "       -5.9857843e-03,  7.3229838e-03,  7.5873756e-03,  8.2963565e-03,\n",
       "       -8.5988473e-03,  2.6364254e-03, -3.5599638e-03,  9.6204039e-03,\n",
       "        2.9037667e-03,  4.6411133e-03,  2.3856140e-03,  6.6084769e-03,\n",
       "       -5.7432912e-03,  7.8944108e-03, -2.4109220e-03, -4.5618867e-03,\n",
       "       -2.0609903e-03,  9.7335577e-03, -6.8565919e-03, -2.1917201e-03,\n",
       "        7.0009995e-03, -5.5749417e-05, -6.2949681e-03, -6.3935257e-03,\n",
       "        8.9403940e-03,  6.4295745e-03,  4.7735930e-03, -3.2620477e-03,\n",
       "       -9.2676207e-03,  3.7868882e-03,  7.1605491e-03, -5.6328895e-03,\n",
       "       -7.8650145e-03, -2.9727411e-03, -4.9318983e-03, -2.3151112e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector=model.wv['growth']\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('achievements', 0.18214833736419678),\n",
       " ('world', 0.17358534038066864),\n",
       " ('history', 0.1671186238527298),\n",
       " ('cheap', 0.15626852214336395),\n",
       " ('account', 0.13278967142105103),\n",
       " ('long', 0.12205980718135834),\n",
       " ('accuracy', 0.12147410213947296),\n",
       " ('leg', 0.11179191619157791),\n",
       " ('stories', 0.11127692461013794),\n",
       " ('people', 0.10941852629184723)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar=model.wv.most_similar('growth')\n",
    "similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('go', 0.24673253297805786),\n",
       " ('remote', 0.18985602259635925),\n",
       " ('online', 0.1783706247806549),\n",
       " ('take', 0.171890988945961),\n",
       " ('used', 0.17061734199523926),\n",
       " ('life', 0.16160978376865387),\n",
       " ('become', 0.1608772575855255),\n",
       " ('computer-based', 0.11331520229578018),\n",
       " ('reading', 0.10772479325532913),\n",
       " ('stories', 0.10574804991483688)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('science')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
